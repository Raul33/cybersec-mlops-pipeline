# ğŸ“Š EvaluaciÃ³n del modelo de detecciÃ³n de anomalÃ­as (Isolation Forest)

## ğŸ¯ Objetivo del mÃ³dulo

Este mÃ³dulo implementa la fase de **evaluaciÃ³n del modelo** dentro del pipeline MLOps, permitiendo analizar el comportamiento del modelo **Isolation Forest** entrenado sobre datos transformados (SILVER).

La evaluaciÃ³n proporciona:

- mÃ©tricas cuantitativas del rendimiento del modelo
- anÃ¡lisis estadÃ­stico del score de anomalÃ­a
- trazabilidad completa entre modelo, dataset y resultados
- persistencia de resultados y auditorÃ­a en PostgreSQL

Este paso completa el ciclo:
**ingesta â†’ transformaciÃ³n â†’ entrenamiento â†’ evaluaciÃ³n**

---

## ğŸ” Flujo funcional de evaluaciÃ³n

El flujo de evaluaciÃ³n se implementa mediante **Prefect** y sigue los siguientes pasos:

1ï¸âƒ£ **Carga del modelo mÃ¡s reciente desde MinIO**  
Se identifica y descarga automÃ¡ticamente el Ãºltimo modelo disponible en el bucket:

```text
cybersec-ml-models
```


2ï¸âƒ£ **Carga del dataset SILVER mÃ¡s reciente**
Se selecciona el Ãºltimo dataset transformado disponible en:

```text
data/silver
```

3ï¸âƒ£ **GeneraciÃ³n de predicciones y scores**
El modelo aplica:

- `decision_function()` â†’ score continuo de anomalÃ­a

- `predict()` â†’ clasificaciÃ³n binaria (0 = normal, 1 = anÃ³malo)

4ï¸âƒ£ **CÃ¡lculo de mÃ©tricas de evaluaciÃ³n**
Se calculan mÃ©tricas agregadas para analizar el comportamiento del modelo.

5ï¸âƒ£ **Persistencia de resultados de evaluaciÃ³n**
Las mÃ©tricas se guardan en formato Parquet y se suben a MinIO.

6ï¸âƒ£ **Registro del evento de evaluaciÃ³n en PostgreSQL**
Se almacena un registro estructurado con trazabilidad completa del proceso.

---

## ğŸ§  ImplementaciÃ³n tÃ©cnica

El flow completo estÃ¡ definido en:

```text
pipeline/evaluation/model_evaluation_flow.py
```

**Tasks principales**

- load_latest_model
Descarga el modelo mÃ¡s reciente desde MinIO.

- load_latest_silver
Carga el dataset SILVER mÃ¡s reciente para evaluaciÃ³n.

- generate_predictions
Aplica el modelo para generar:

    - `anomaly_score`

    - `prediction` (0 = normal, 1 = anÃ³malo)

- compute_metrics
Calcula mÃ©tricas agregadas de evaluaciÃ³n.

- save_eval_results
Guarda los resultados de evaluaciÃ³n como Parquet local.

- upload_eval_to_minio
Sube el fichero de evaluaciÃ³n al bucket correspondiente.

- register_eval_event
Registra el evento de evaluaciÃ³n en PostgreSQL.

---

## ğŸ“Š MÃ©tricas calculadas

Dado que el problema es **no supervisado** y no existe ground truth real, se asume un escenario de normalidad dominante para poder derivar mÃ©tricas operativas.

Las mÃ©tricas calculadas son:

- **precision â†’** proporciÃ³n de detecciones correctas

- **recall â†’** capacidad de detecciÃ³n de anomalÃ­as

- **f1 â†’** equilibrio entre precisiÃ³n y recall

- **anomaly_rate â†’** proporciÃ³n de eventos marcados como anÃ³malos

- **score_mean â†’** media del score de anomalÃ­a

- **score_std â†’** desviaciÃ³n estÃ¡ndar del score

Estas mÃ©tricas permiten:

- validar estabilidad del modelo

- detectar posibles derivas

- comparar ejecuciones entre sÃ­

---

## ğŸ“ Estructura de artefactos generados

### ğŸ“¦ Resultados de evaluaciÃ³n (local)

```text
data/eval/eval_<timestamp>.parquet
```

### ğŸ“¦ Almacenamiento en MinIO

```text
s3://cybersec-ml-eval/eval_<timestamp>.parquet
```

---

## ğŸ—„ï¸ AuditorÃ­a en PostgreSQL

Cada ejecuciÃ³n genera un registro en la tabla `evaluation_events`.

**Esquema de la tabla**

```sql
CREATE TABLE evaluation_events (
    id SERIAL PRIMARY KEY,
    timestamp_eval TIMESTAMP NOT NULL,
    modelo_nombre TEXT NOT NULL,
    nombre_dataset TEXT NOT NULL,
    ruta_resultados TEXT NOT NULL,
    metrics JSONB NOT NULL,
    estado TEXT NOT NULL
);
```
**InformaciÃ³n registrada**

- modelo evaluado

- dataset utilizado

- ubicaciÃ³n del resultado

- mÃ©tricas calculadas (JSON)

- estado de la ejecuciÃ³n

---

## ğŸ“‚ Estructura del mÃ³dulo

```text
pipeline/
  evaluation/
    model_evaluation_flow.py
    README.md
```

---

## â–¶ï¸ EjecuciÃ³n del flow


```bash
python pipeline/evaluation/model_evaluation_flow.py
```

Ejemplo de salida real:


```text
EvaluaciÃ³n subida correctamente: s3://cybersec-ml-eval/eval_20251224_183029.parquet
Evento de evaluaciÃ³n registrado.
EvaluaciÃ³n completada.
Flow run 'imported-rhino' - Finished in state Completed()
```

---

## ğŸ”— IntegraciÃ³n en el pipeline MLOps completo

Este mÃ³dulo se ejecuta como **subflow** dentro del pipeline global definido en:

```text
pipeline/full_mlops_flow.py
```

Su ejecuciÃ³n permite:

- validar el modelo entrenado

- generar evidencias cuantitativas

- alimentar decisiones de despliegue o retraining

- cerrar el ciclo de vida MLOps del sistema

---

## ğŸ“Œ Resultados alcanzados

âœ” evaluaciÃ³n automatizada y reproducible
âœ” mÃ©tricas cuantitativas persistidas
âœ” trazabilidad modeloâ€“datosâ€“resultados
âœ” almacenamiento desacoplado en MinIO
âœ” auditorÃ­a completa en PostgreSQL
âœ” cierre del pipeline MLOps end-to-end